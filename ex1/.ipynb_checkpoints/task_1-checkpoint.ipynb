{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a0079e8",
   "metadata": {},
   "source": [
    "# Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54d26783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/pablo/opt/anaconda3/envs/UOL_sp_openCV/lib/python3.9/site-packages (4.7.0.68)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /Users/pablo/opt/anaconda3/envs/UOL_sp_openCV/lib/python3.9/site-packages (from opencv-python) (1.24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85566f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b94033",
   "metadata": {},
   "source": [
    "# Select which video to analize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed2e82b",
   "metadata": {},
   "source": [
    "Replace INPUT_VIDEO_PATH var below with the name of video you want to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4775b61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_VIDEO_PATH = \"input_files/Traffic_Laramie_1.mp4\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb227b0",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4416998",
   "metadata": {},
   "source": [
    "The below function extracts the background from a video and returns a frame with all moving objects removed. To do this it obtains the median over 50 random frames selected from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e89e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_background(file_path):\n",
    "    cap = cv2.VideoCapture(file_path)\n",
    "    # we will randomly select 50 frames for the calculating the median\n",
    "    frame_indices = cap.get(cv2.CAP_PROP_FRAME_COUNT) * np.random.uniform(size=50)\n",
    "    # we will store the frames in array\n",
    "    frames = []\n",
    "    for idx in frame_indices:\n",
    "        # set the frame id to read that particular frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        frames.append(frame)\n",
    "    # calculate the median\n",
    "    median_frame = np.median(frames, axis=0).astype(np.uint8)\n",
    "    return median_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c938ff",
   "metadata": {},
   "source": [
    "# Read the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c186d59f",
   "metadata": {},
   "source": [
    "## Hyperparameters used for motion detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f7fa5",
   "metadata": {},
   "source": [
    "Below we define the hyperparameters used by the algorithm.  \n",
    "CONSECUTIVE_FRAMES is the number of consecutive frames we will analize together to finding object contours, incrementing it will reduce computation but drawing boxes will be less accurate.  \n",
    "MINIMAL_SIZE: minimal area in motion to be detected, keep it high enough to avoid detecting pedestrians and cars that are far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7767033",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSECUTIVE_FRAMES = 2 # how manny frames will be compared to detect movement, less = more detailed detection, more computation needed\n",
    "MINIMAL_SIZE = 2000 # minimal object size, big enough to detect cars but avoid pedestrians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577fc488",
   "metadata": {},
   "source": [
    "## read video and instantiate video writer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d7aea",
   "metadata": {},
   "source": [
    "Reads input video and extracts width, height and fps.\n",
    "then uses those parameters to instantiate a video writer that will be used to write the updated frames to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83c5df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n",
    "# get the video frame height and width\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "fps = int(cap.get(5))\n",
    "\n",
    "save_name = f\"output_files_task1/{INPUT_VIDEO_PATH.split('/')[-1]}\"\n",
    "# define codec and create VideoWriter object\n",
    "out = cv2.VideoWriter(\n",
    "    save_name,\n",
    "    cv2.VideoWriter_fourcc(*'mp4v'), fps, \n",
    "    (frame_width, frame_height)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "581ae0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c450c2",
   "metadata": {},
   "source": [
    "input video runs at 25 fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dd0057",
   "metadata": {},
   "source": [
    "# Get background frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd5745",
   "metadata": {},
   "source": [
    "Applies background substraction technique to obtain a frame with moving objects removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3ffe7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the background model\n",
    "background = get_background(INPUT_VIDEO_PATH)\n",
    "# convert the background model to grayscale format\n",
    "background = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
    "frame_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a62d94",
   "metadata": {},
   "source": [
    "# Loop over video frames and detect moving objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bac17",
   "metadata": {},
   "source": [
    "Loop over input video frames and do the following:  \n",
    "1- convert frames to grayscale for easier detection  \n",
    "2- find difference between current frame and background frame, ie: remove the background.  \n",
    "3- apply a threshold to convert all pixels to either 1 or 0 depending on difference with background.  \n",
    "4- sum consecutive frames(with background removed): find contours around objects, represented as group of pixels clustered together with values above 0, only reason to do this is to reduce computation. finding the contour on a sequence of CONSECUTIVE_FRAMES frames is CONSECUTIVE_FRAMES times faster than doing it for individual frames.  \n",
    "5- Use OpenCV's findContours function to find contours for consecutive pixels that have non 0 values. We only find contours for the last pixel of each group of CONSECUTIVE_FRAMES, since need the previous frames of the group to calculate it(Later I still use the same contour for every pixel in the group)  \n",
    "6- append each frame with detected contours(if available) to a list of frames to later be written to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "821c6bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_diff_list = []\n",
    "frames_to_write = []\n",
    "while (cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        frame_count += 1\n",
    "        orig_frame = frame.copy()\n",
    "        # IMPORTANT STEP: convert the frame to grayscale first\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        if frame_count % CONSECUTIVE_FRAMES == 0 or frame_count == 1:\n",
    "            frame_diff_list = []\n",
    "        # find the difference between current frame and base frame\n",
    "        frame_diff = cv2.absdiff(gray, background)\n",
    "        # thresholding to convert the frame to binary\n",
    "        ret, thres = cv2.threshold(frame_diff, 50, 255, cv2.THRESH_BINARY)\n",
    "        # dilate the frame a bit to get some more white area...\n",
    "        # ... makes the detection of contours a bit easier\n",
    "        dilate_frame = cv2.dilate(thres, None, iterations=2)\n",
    "        # append the final result into the `frame_diff_list`\n",
    "        frame_diff_list.append(dilate_frame)\n",
    "        # if we have reached `CONSECUTIVE_FRAMES` number of frames\n",
    "        if len(frame_diff_list) == CONSECUTIVE_FRAMES:\n",
    "            # add all the frames in the `frame_diff_list`\n",
    "            sum_frames = sum(frame_diff_list)\n",
    "            # find the contours around the white segmented areas\n",
    "            contours, hierarchy = cv2.findContours(sum_frames, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            frames_to_write.append((orig_frame, contours))\n",
    "        else:\n",
    "            # write intermediate frames to avoid frame skipping in output video? \n",
    "            frames_to_write.append((orig_frame, []))\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3326ad5",
   "metadata": {},
   "source": [
    "# write frames with the contours to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a02a8",
   "metadata": {},
   "source": [
    "Loop over frames, contours list processed in previous step and for each frame:\n",
    "1) If don't have contours for this image, find the next contour in the list\n",
    "2) draw all contours as rectangles in the frame\n",
    "2) Write frame to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5300a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for frame, contours in frames_to_write:\n",
    "    j=1\n",
    "    while(not contours and (i+j) <len(frames_to_write)):\n",
    "        contours = frames_to_write[i+j][1]\n",
    "        j+=1\n",
    "    i+=1\n",
    "    for contour in contours:\n",
    "        # continue through the loop if contour area is less than MINIMAL_SIZE...\n",
    "        # ... helps in removing noise detection\n",
    "        if cv2.contourArea(contour) < MINIMAL_SIZE:\n",
    "            continue\n",
    "        # get the xmin, ymin, width, and height coordinates from the contours\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        # draw the bounding boxes\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    out.write(frame)\n",
    "    \n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a122059",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e500669",
   "metadata": {},
   "source": [
    "Some of the ideas I used to implement my application are based on the following articles:\n",
    "- https://research.ijcaonline.org/volume102/number7/pxc3898647.pdf\n",
    "- https://debuggercafe.com/moving-object-detection-using-frame-differencing-with-opencv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807fdcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:UOL_sp_openCV] *",
   "language": "python",
   "name": "conda-env-UOL_sp_openCV-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
